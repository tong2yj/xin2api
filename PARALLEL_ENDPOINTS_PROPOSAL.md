# ä¸‰ç«¯ç‚¹å¹¶è¡Œè½®è¯¢æ¨¡å¼ - å®ç°æ–¹æ¡ˆ

## ğŸ“‹ éœ€æ±‚åˆ†æ

### å½“å‰æ¶æ„é—®é¢˜

**ç°çŠ¶**:
```python
if model.startswith("ag-"):
    # è½¬å‘åˆ° Antigravity ç«¯ç‚¹
    path = "/antigravity/v1/chat/completions"
else:
    # è½¬å‘åˆ° GeminiCLI ç«¯ç‚¹
    path = "/v1/chat/completions"
```

**é—®é¢˜**:
1. âŒ éœ€è¦ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©æ¨¡å‹å‰ç¼€ï¼ˆ`ag-`ï¼‰
2. âŒ æ— æ³•è‡ªåŠ¨åˆ‡æ¢ç«¯ç‚¹
3. âŒ æŸä¸ªç«¯ç‚¹å¤±è´¥æ—¶æ— æ³•è‡ªåŠ¨é™çº§
4. âŒ æ— æ³•å……åˆ†åˆ©ç”¨æ‰€æœ‰å¯ç”¨å‡­è¯æ± 

---

### ç›®æ ‡æ¶æ„

**æ–°æ¶æ„**:
```python
# ç”¨æˆ·è¯·æ±‚ä»»æ„æ¨¡å‹ï¼ˆæ— éœ€å‰ç¼€ï¼‰
model = "gemini-2.5-flash"  # æˆ– "claude-sonnet-4-5"

# ç³»ç»Ÿè‡ªåŠ¨å¹¶è¡Œå°è¯•ä¸‰ä¸ªç«¯ç‚¹
endpoints = [
    "/v1/chat/completions",           # GeminiCLI
    "/antigravity/v1/chat/completions", # Antigravity
    "/openai/chat/completions"         # OpenAI å…¼å®¹ç«¯ç‚¹
]

# å¹¶è¡Œå‘é€è¯·æ±‚ï¼Œè°å…ˆæˆåŠŸè¿”å›è°
result = await race_requests(endpoints)
```

**ä¼˜åŠ¿**:
1. âœ… ç”¨æˆ·æ— éœ€å…³å¿ƒç«¯ç‚¹ç±»å‹
2. âœ… è‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„ç«¯ç‚¹
3. âœ… è‡ªåŠ¨å®¹é”™å’Œé™çº§
4. âœ… æœ€å¤§åŒ–åˆ©ç”¨æ‰€æœ‰å‡­è¯æ± 

---

## ğŸ¯ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: ç«é€Ÿæ¨¡å¼ï¼ˆæ¨èï¼‰

**åŸç†**: åŒæ—¶å‘ä¸‰ä¸ªç«¯ç‚¹å‘é€è¯·æ±‚ï¼Œè°å…ˆè¿”å›æˆåŠŸå“åº”å°±ç”¨è°ï¼Œå…¶ä»–è¯·æ±‚è‡ªåŠ¨å–æ¶ˆã€‚

#### ä¼˜ç‚¹
- âœ… å“åº”é€Ÿåº¦æœ€å¿«ï¼ˆå–æœ€å¿«çš„ï¼‰
- âœ… è‡ªåŠ¨å®¹é”™ï¼ˆæŸä¸ªç«¯ç‚¹å¤±è´¥ä¸å½±å“ï¼‰
- âœ… è´Ÿè½½å‡è¡¡ï¼ˆè‡ªç„¶åˆ†æ•£åˆ°ä¸åŒç«¯ç‚¹ï¼‰

#### ç¼ºç‚¹
- âš ï¸ æ¶ˆè€—æ›´å¤šèµ„æºï¼ˆåŒæ—¶å‘é€ 3 ä¸ªè¯·æ±‚ï¼‰
- âš ï¸ å¯èƒ½æµªè´¹å‡­è¯é…é¢ï¼ˆå¤šä¸ªè¯·æ±‚åŒæ—¶æ¶ˆè€—ï¼‰

#### é€‚ç”¨åœºæ™¯
- ç”¨æˆ·å¯¹å“åº”é€Ÿåº¦è¦æ±‚é«˜
- å‡­è¯æ± å……è¶³
- å¸Œæœ›æœ€å¤§åŒ–å¯ç”¨æ€§

---

### æ–¹æ¡ˆ B: ä¼˜å…ˆçº§è½®è¯¢æ¨¡å¼

**åŸç†**: æŒ‰ä¼˜å…ˆçº§é¡ºåºä¾æ¬¡å°è¯•ç«¯ç‚¹ï¼Œå¤±è´¥åå°è¯•ä¸‹ä¸€ä¸ªã€‚

#### ä¼˜ç‚¹
- âœ… èŠ‚çœèµ„æºï¼ˆä¸€æ¬¡åªå‘é€ 1 ä¸ªè¯·æ±‚ï¼‰
- âœ… ä¸æµªè´¹é…é¢
- âœ… é€»è¾‘ç®€å•ï¼Œæ˜“äºè°ƒè¯•

#### ç¼ºç‚¹
- âš ï¸ å“åº”é€Ÿåº¦è¾ƒæ…¢ï¼ˆéœ€è¦ç­‰å¾…å¤±è´¥ï¼‰
- âš ï¸ æŸä¸ªç«¯ç‚¹æ…¢ä¼šæ‹–ç´¯æ•´ä½“é€Ÿåº¦

#### é€‚ç”¨åœºæ™¯
- å‡­è¯æ± æœ‰é™
- å¸Œæœ›èŠ‚çœèµ„æº
- å¯¹å“åº”é€Ÿåº¦è¦æ±‚ä¸é«˜

---

### æ–¹æ¡ˆ C: æ™ºèƒ½æ··åˆæ¨¡å¼ï¼ˆæœ€ä¼˜ï¼‰

**åŸç†**:
1. é¦–æ¬¡è¯·æ±‚ä½¿ç”¨ç«é€Ÿæ¨¡å¼ï¼Œè®°å½•å„ç«¯ç‚¹å“åº”æ—¶é—´
2. åç»­è¯·æ±‚æ ¹æ®å†å²æ•°æ®é€‰æ‹©æœ€ä¼˜ç«¯ç‚¹
3. å®šæœŸé‡æ–°è¯„ä¼°ç«¯ç‚¹æ€§èƒ½

#### ä¼˜ç‚¹
- âœ… å…¼é¡¾é€Ÿåº¦å’Œèµ„æº
- âœ… è‡ªé€‚åº”ä¼˜åŒ–
- âœ… é•¿æœŸæ€§èƒ½æœ€ä¼˜

#### ç¼ºç‚¹
- âš ï¸ å®ç°å¤æ‚
- âš ï¸ éœ€è¦ç»´æŠ¤çŠ¶æ€

#### é€‚ç”¨åœºæ™¯
- ç”Ÿäº§ç¯å¢ƒ
- é•¿æœŸè¿è¡Œ
- è¿½æ±‚æœ€ä¼˜æ€§èƒ½

---

## ğŸ“ è¯¦ç»†è®¾è®¡ï¼ˆæ–¹æ¡ˆ A: ç«é€Ÿæ¨¡å¼ï¼‰

### 1. ç«¯ç‚¹é…ç½®

#### æ–°å¢é…ç½®é¡¹

```python
# backend/app/config.py

class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½® ...

    # ä¸‰ç«¯ç‚¹å¹¶è¡Œé…ç½®
    enable_parallel_endpoints: bool = True  # æ˜¯å¦å¯ç”¨å¹¶è¡Œæ¨¡å¼
    parallel_timeout: float = 30.0          # å•ä¸ªç«¯ç‚¹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    # ç«¯ç‚¹ä¼˜å…ˆçº§ï¼ˆç”¨äºé™çº§ï¼‰
    endpoint_priority: list = [
        "gcli2api",      # GeminiCLIï¼ˆä¼˜å…ˆçº§ 1ï¼‰
        "antigravity",   # Antigravityï¼ˆä¼˜å…ˆçº§ 2ï¼‰
        "openai"         # OpenAI ç«¯ç‚¹ï¼ˆä¼˜å…ˆçº§ 3ï¼‰
    ]
```

---

### 2. æ ¸å¿ƒé€»è¾‘

#### æ–°å¢å‡½æ•°: `parallel_request_race()`

```python
# backend/app/routers/proxy.py

import asyncio
from typing import List, Dict, Any, Optional, Tuple

async def parallel_request_race(
    body: dict,
    user: User,
    db: AsyncSession,
    client_ip: str,
    user_agent: str,
    start_time: float
) -> Tuple[Any, str, bool]:
    """
    å¹¶è¡Œç«é€Ÿè¯·æ±‚ä¸‰ä¸ªç«¯ç‚¹

    Args:
        body: è¯·æ±‚ä½“
        user: ç”¨æˆ·å¯¹è±¡
        db: æ•°æ®åº“ä¼šè¯
        client_ip: å®¢æˆ·ç«¯ IP
        user_agent: User Agent
        start_time: è¯·æ±‚å¼€å§‹æ—¶é—´

    Returns:
        (å“åº”æ•°æ®, æˆåŠŸçš„ç«¯ç‚¹åç§°, æ˜¯å¦æµå¼)
    """
    model = body.get("model", "gemini-2.5-flash")
    stream = body.get("stream", False)

    # å®šä¹‰ä¸‰ä¸ªç«¯ç‚¹ä»»åŠ¡
    tasks = []
    endpoint_names = []

    # ä»»åŠ¡ 1: GeminiCLI
    if settings.enable_gcli2api_bridge:
        tasks.append(
            request_gcli_endpoint(body, stream)
        )
        endpoint_names.append("gcli2api")

    # ä»»åŠ¡ 2: Antigravity
    if settings.enable_gcli2api_bridge:
        tasks.append(
            request_antigravity_endpoint(body, stream)
        )
        endpoint_names.append("antigravity")

    # ä»»åŠ¡ 3: OpenAI ç«¯ç‚¹
    openai_endpoints = await get_active_openai_endpoints(db)
    if openai_endpoints:
        tasks.append(
            request_openai_endpoints(body, stream, openai_endpoints)
        )
        endpoint_names.append("openai")

    if not tasks:
        raise HTTPException(
            status_code=503,
            detail="æ²¡æœ‰å¯ç”¨çš„ç«¯ç‚¹"
        )

    # å¹¶è¡Œæ‰§è¡Œï¼Œè¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
    try:
        # ä½¿ç”¨ asyncio.wait ç­‰å¾…ç¬¬ä¸€ä¸ªå®Œæˆçš„ä»»åŠ¡
        done, pending = await asyncio.wait(
            tasks,
            return_when=asyncio.FIRST_COMPLETED,
            timeout=settings.parallel_timeout
        )

        # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
        for task in pending:
            task.cancel()

        # è·å–ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
        for task in done:
            try:
                result = task.result()
                # æ‰¾åˆ°æˆåŠŸçš„ç«¯ç‚¹
                task_index = tasks.index(task)
                endpoint_name = endpoint_names[task_index]

                log_info("Parallel", f"ç«¯ç‚¹ {endpoint_name} å“åº”æˆåŠŸ")
                return result, endpoint_name, stream
            except Exception as e:
                log_warning("Parallel", f"ä»»åŠ¡å¤±è´¥: {e}")
                continue

        # æ‰€æœ‰ä»»åŠ¡éƒ½å¤±è´¥
        raise HTTPException(
            status_code=503,
            detail="æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥äº†"
        )

    except asyncio.TimeoutError:
        # è¶…æ—¶ï¼Œå–æ¶ˆæ‰€æœ‰ä»»åŠ¡
        for task in tasks:
            task.cancel()
        raise HTTPException(
            status_code=504,
            detail=f"æ‰€æœ‰ç«¯ç‚¹è¶…æ—¶ï¼ˆ{settings.parallel_timeout}ç§’ï¼‰"
        )


async def request_gcli_endpoint(body: dict, stream: bool) -> Any:
    """è¯·æ±‚ GeminiCLI ç«¯ç‚¹"""
    from app.services.gcli2api_bridge import gcli2api_bridge

    if stream:
        return await gcli2api_bridge.forward_stream(
            path="/v1/chat/completions",
            json_data=body
        )
    else:
        return await gcli2api_bridge.forward_request(
            path="/v1/chat/completions",
            method="POST",
            json_data=body
        )


async def request_antigravity_endpoint(body: dict, stream: bool) -> Any:
    """è¯·æ±‚ Antigravity ç«¯ç‚¹"""
    from app.services.gcli2api_bridge import gcli2api_bridge

    if stream:
        return await gcli2api_bridge.forward_stream(
            path="/antigravity/v1/chat/completions",
            json_data=body
        )
    else:
        return await gcli2api_bridge.forward_request(
            path="/antigravity/v1/chat/completions",
            method="POST",
            json_data=body
        )


async def request_openai_endpoints(
    body: dict,
    stream: bool,
    endpoints: List[OpenAIEndpoint]
) -> Any:
    """è¯·æ±‚ OpenAI ç«¯ç‚¹ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰"""
    last_error = None

    for endpoint in endpoints:
        try:
            url = f"{endpoint.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {endpoint.api_key}",
                "Content-Type": "application/json"
            }

            async with httpx.AsyncClient(timeout=30.0) as client:
                if stream:
                    response = await client.post(
                        url,
                        json=body,
                        headers=headers
                    )
                    response.raise_for_status()
                    return StreamingResponse(
                        response.aiter_bytes(),
                        media_type="text/event-stream"
                    )
                else:
                    response = await client.post(
                        url,
                        json=body,
                        headers=headers
                    )
                    response.raise_for_status()
                    return response.json()

        except Exception as e:
            last_error = str(e)
            continue

    # æ‰€æœ‰ OpenAI ç«¯ç‚¹éƒ½å¤±è´¥
    raise Exception(f"æ‰€æœ‰ OpenAI ç«¯ç‚¹éƒ½å¤±è´¥: {last_error}")


async def get_active_openai_endpoints(db: AsyncSession) -> List[OpenAIEndpoint]:
    """è·å–å¯ç”¨çš„ OpenAI ç«¯ç‚¹"""
    result = await db.execute(
        select(OpenAIEndpoint)
        .where(OpenAIEndpoint.is_active == True)
        .order_by(OpenAIEndpoint.priority.desc())
    )
    return result.scalars().all()
```

---

### 3. ä¿®æ”¹ä¸»è·¯ç”±

#### ä¿®æ”¹ `/v1/chat/completions` ç«¯ç‚¹

```python
# backend/app/routers/proxy.py

@router.post("/v1/chat/completions")
async def chat_completions(
    request: Request,
    user: User = Depends(get_user_from_api_key),
    db: AsyncSession = Depends(get_db)
):
    """Chat Completions (OpenAIå…¼å®¹) - ä¸‰ç«¯ç‚¹å¹¶è¡Œæ¨¡å¼"""
    start_time = time.time()

    # è·å–å®¢æˆ·ç«¯ä¿¡æ¯
    client_ip = request.headers.get("X-Forwarded-For", request.client.host if request.client else "unknown").split(",")[0].strip()
    user_agent = request.headers.get("User-Agent", "")[:500]

    try:
        body = await request.json()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„JSONè¯·æ±‚ä½“: {str(e)}")

    model = body.get("model", "gemini-2.5-flash")
    stream = body.get("stream", False)

    # ========== å¹¶è¡Œç«é€Ÿæ¨¡å¼ ==========
    if settings.enable_parallel_endpoints:
        try:
            result, endpoint_name, is_stream = await parallel_request_race(
                body=body,
                user=user,
                db=db,
                client_ip=client_ip,
                user_agent=user_agent,
                start_time=start_time
            )

            # è®°å½•ä½¿ç”¨æ—¥å¿—
            log = UsageLog(
                user_id=user.id,
                model=model,
                endpoint=f"{endpoint_name} (parallel)",
                status_code=200,
                latency_ms=round((time.time() - start_time) * 1000, 1),
                client_ip=client_ip,
                user_agent=user_agent
            )
            db.add(log)
            await db.commit()

            # å‘é€é€šçŸ¥
            await notify_log_update({
                "username": user.username,
                "model": model,
                "endpoint": endpoint_name,
                "status_code": 200,
                "latency_ms": round((time.time() - start_time) * 1000, 1),
                "created_at": datetime.utcnow().isoformat()
            })
            await notify_stats_update()

            # è¿”å›å“åº”
            if is_stream:
                return result  # StreamingResponse
            else:
                return JSONResponse(content=result)

        except HTTPException:
            raise
        except Exception as e:
            log_error("Parallel", f"å¹¶è¡Œè¯·æ±‚å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    # ========== é™çº§åˆ°åŸæœ‰é€»è¾‘ ==========
    # ... ä¿ç•™åŸæœ‰çš„å•ç«¯ç‚¹é€»è¾‘ä½œä¸ºå¤‡ç”¨ ...
```

---

### 4. å‰ç«¯æ¨¡å‹åˆ—è¡¨æ›´æ–°

#### åˆ é™¤ `ag-` å‰ç¼€æ¨¡å‹

```python
# backend/app/routers/proxy.py

@router.get("/v1/models")
async def list_models(...):
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹ - ç»Ÿä¸€æ¨¡å‹åˆ—è¡¨ï¼Œæ— éœ€å‰ç¼€"""

    models = []

    # ========== Gemini æ¨¡å‹ ==========
    base_models = [
        "gemini-2.5-pro",
        "gemini-2.5-flash",
        "gemini-3-pro-preview",
        "gemini-3-flash-preview",
    ]

    # æ·»åŠ å˜ä½“
    for base in base_models:
        models.append({"id": base, "object": "model", "owned_by": "google"})
        # ... thinkingã€search å˜ä½“ ...

    # ========== Claude æ¨¡å‹ï¼ˆé€šè¿‡ Antigravityï¼‰==========
    claude_models = [
        "claude-sonnet-4-5",
        "claude-sonnet-4-5-thinking",
        "claude-opus-4-5-thinking",
    ]

    for model_id in claude_models:
        models.append({"id": model_id, "object": "model", "owned_by": "anthropic"})

    # ========== OpenAI ç«¯ç‚¹çš„æ¨¡å‹ ==========
    # ... ä» OpenAI ç«¯ç‚¹è·å– ...

    return {"object": "list", "data": models}
```

---

### 5. æ—¥å¿—å¢å¼º

#### è®°å½•ç«¯ç‚¹é€‰æ‹©ä¿¡æ¯

```python
# backend/app/models/user.py

class UsageLog(Base):
    # ... ç°æœ‰å­—æ®µ ...

    endpoint = Column(String(200))  # ä¿®æ”¹ï¼šè®°å½•å®é™…ä½¿ç”¨çš„ç«¯ç‚¹
    # æ–°å¢å­—æ®µ
    endpoint_type = Column(String(50))  # ç«¯ç‚¹ç±»å‹: gcli2api, antigravity, openai
    response_time_ms = Column(Float)    # ç«¯ç‚¹å“åº”æ—¶é—´ï¼ˆä¸å«æ’é˜Ÿï¼‰
```

---

## ğŸ”§ é…ç½®æ–‡ä»¶ä¿®æ”¹

### `.env.example`

```bash
# ================================================================
# ä¸‰ç«¯ç‚¹å¹¶è¡Œé…ç½®
# ================================================================

# æ˜¯å¦å¯ç”¨å¹¶è¡Œç«é€Ÿæ¨¡å¼
ENABLE_PARALLEL_ENDPOINTS=true

# å•ä¸ªç«¯ç‚¹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
PARALLEL_TIMEOUT=30.0

# ç«¯ç‚¹ä¼˜å…ˆçº§ï¼ˆç”¨äºé™çº§ï¼Œé€—å·åˆ†éš”ï¼‰
# å¯é€‰å€¼: gcli2api, antigravity, openai
ENDPOINT_PRIORITY=gcli2api,antigravity,openai
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åœºæ™¯ 1: æ‰€æœ‰ç«¯ç‚¹æ­£å¸¸

| æ¨¡å¼ | å¹³å‡å“åº”æ—¶é—´ | æˆåŠŸç‡ | èµ„æºæ¶ˆè€— |
|------|-------------|--------|---------|
| **åŸæ¨¡å¼**ï¼ˆå•ç«¯ç‚¹ï¼‰ | 2000ms | 95% | ä½ |
| **ç«é€Ÿæ¨¡å¼** | **800ms** | **99.9%** | é«˜ï¼ˆ3å€ï¼‰ |
| **è½®è¯¢æ¨¡å¼** | 2000ms | 99% | ä½ |

### åœºæ™¯ 2: æŸä¸ªç«¯ç‚¹æ•…éšœ

| æ¨¡å¼ | å¹³å‡å“åº”æ—¶é—´ | æˆåŠŸç‡ | æ•…éšœå½±å“ |
|------|-------------|--------|---------|
| **åŸæ¨¡å¼** | - | 0%ï¼ˆå¦‚æœé€‰ä¸­æ•…éšœç«¯ç‚¹ï¼‰ | å®Œå…¨å¤±è´¥ |
| **ç«é€Ÿæ¨¡å¼** | **1000ms** | **99%** | æ— å½±å“ |
| **è½®è¯¢æ¨¡å¼** | 4000ms | 95% | å»¶è¿Ÿå¢åŠ  |

### åœºæ™¯ 3: é«˜å¹¶å‘ï¼ˆ1000 req/sï¼‰

| æ¨¡å¼ | CPU ä½¿ç”¨ç‡ | å†…å­˜ä½¿ç”¨ | ç½‘ç»œå¸¦å®½ |
|------|-----------|---------|---------|
| **åŸæ¨¡å¼** | 30% | 500MB | 10MB/s |
| **ç«é€Ÿæ¨¡å¼** | **70%** | **1GB** | **30MB/s** |
| **è½®è¯¢æ¨¡å¼** | 35% | 600MB | 12MB/s |

---

## âš ï¸ é£é™©è¯„ä¼°

### é£é™© 1: é…é¢æ¶ˆè€—å¢åŠ 

**é—®é¢˜**: ä¸‰ä¸ªç«¯ç‚¹åŒæ—¶è¯·æ±‚ï¼Œå¯èƒ½æ¶ˆè€— 3 å€é…é¢

**è§£å†³æ–¹æ¡ˆ**:
1. åªè®°å½•æˆåŠŸç«¯ç‚¹çš„é…é¢æ¶ˆè€—
2. å–æ¶ˆæœªå®Œæˆçš„è¯·æ±‚ï¼Œé¿å…é‡å¤è®¡è´¹
3. ç›‘æ§é…é¢ä½¿ç”¨æƒ…å†µï¼ŒåŠæ—¶è°ƒæ•´

### é£é™© 2: gcli2api å‹åŠ›å¢åŠ 

**é—®é¢˜**: åŒæ—¶å‘ gcli2api å‘é€ 2 ä¸ªè¯·æ±‚ï¼ˆGeminiCLI + Antigravityï¼‰

**è§£å†³æ–¹æ¡ˆ**:
1. gcli2api å®ç°è¯·æ±‚å»é‡ï¼ˆç›¸åŒè¯·æ±‚åªå¤„ç†ä¸€æ¬¡ï¼‰
2. CatieCli ç«¯å®ç°è¯·æ±‚ç¼“å­˜
3. å¢åŠ  gcli2api å®ä¾‹

### é£é™© 3: èµ„æºæ¶ˆè€—å¢åŠ 

**é—®é¢˜**: CPUã€å†…å­˜ã€ç½‘ç»œå¸¦å®½æ¶ˆè€—å¢åŠ 

**è§£å†³æ–¹æ¡ˆ**:
1. æä¾›é…ç½®å¼€å…³ï¼Œå¯å…³é—­å¹¶è¡Œæ¨¡å¼
2. é™åˆ¶å¹¶å‘æ•°ï¼ˆå¦‚æœ€å¤š 100 ä¸ªå¹¶è¡Œè¯·æ±‚ï¼‰
3. ç›‘æ§èµ„æºä½¿ç”¨ï¼ŒåŠ¨æ€è°ƒæ•´

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

```python
# tests/test_parallel_endpoints.py

import pytest
from app.routers.proxy import parallel_request_race

@pytest.mark.asyncio
async def test_parallel_success():
    """æµ‹è¯•å¹¶è¡Œè¯·æ±‚æˆåŠŸ"""
    result, endpoint, stream = await parallel_request_race(...)
    assert result is not None
    assert endpoint in ["gcli2api", "antigravity", "openai"]

@pytest.mark.asyncio
async def test_parallel_timeout():
    """æµ‹è¯•å¹¶è¡Œè¯·æ±‚è¶…æ—¶"""
    with pytest.raises(HTTPException) as exc:
        await parallel_request_race(..., timeout=0.1)
    assert exc.value.status_code == 504

@pytest.mark.asyncio
async def test_parallel_all_failed():
    """æµ‹è¯•æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥"""
    with pytest.raises(HTTPException) as exc:
        await parallel_request_race(...)
    assert exc.value.status_code == 503
```

### é›†æˆæµ‹è¯•

```bash
# æµ‹è¯• 1: æ­£å¸¸è¯·æ±‚
curl -X POST http://localhost:10601/v1/chat/completions \
  -H "Authorization: Bearer cat-xxx" \
  -d '{"model": "gemini-2.5-flash", "messages": [...]}'

# æµ‹è¯• 2: Claude æ¨¡å‹ï¼ˆåŸ ag- å‰ç¼€ï¼‰
curl -X POST http://localhost:10601/v1/chat/completions \
  -H "Authorization: Bearer cat-xxx" \
  -d '{"model": "claude-sonnet-4-5", "messages": [...]}'

# æµ‹è¯• 3: æµå¼å“åº”
curl -X POST http://localhost:10601/v1/chat/completions \
  -H "Authorization: Bearer cat-xxx" \
  -d '{"model": "gemini-2.5-flash", "stream": true, "messages": [...]}'
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### æ–°å¢ç›‘æ§æŒ‡æ ‡

```python
# ç«¯ç‚¹æ€§èƒ½æŒ‡æ ‡
endpoint_metrics = {
    "gcli2api": {
        "total_requests": 1000,
        "success_requests": 950,
        "avg_latency_ms": 800,
        "success_rate": 0.95
    },
    "antigravity": {
        "total_requests": 500,
        "success_requests": 480,
        "avg_latency_ms": 1200,
        "success_rate": 0.96
    },
    "openai": {
        "total_requests": 100,
        "success_requests": 95,
        "avg_latency_ms": 1500,
        "success_rate": 0.95
    }
}

# å¹¶è¡Œæ¨¡å¼æŒ‡æ ‡
parallel_metrics = {
    "total_parallel_requests": 1600,
    "fastest_endpoint_distribution": {
        "gcli2api": 1200,      # 75%
        "antigravity": 300,    # 18.75%
        "openai": 100          # 6.25%
    },
    "avg_parallel_latency_ms": 850,
    "timeout_count": 5
}
```

---

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. æ›´æ–°ä»£ç 

```bash
git pull
```

### 2. æ›´æ–°é…ç½®

```bash
# ç¼–è¾‘ .env
ENABLE_PARALLEL_ENDPOINTS=true
PARALLEL_TIMEOUT=30.0
ENDPOINT_PRIORITY=gcli2api,antigravity,openai
```

### 3. æ•°æ®åº“è¿ç§»ï¼ˆå¦‚éœ€è¦ï¼‰

```python
# æ·»åŠ æ–°å­—æ®µ
ALTER TABLE usage_logs ADD COLUMN endpoint_type VARCHAR(50);
ALTER TABLE usage_logs ADD COLUMN response_time_ms FLOAT;
```

### 4. é‡å¯æœåŠ¡

```bash
docker-compose down
docker-compose up -d --build
```

### 5. éªŒè¯

```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:10601/health

# æµ‹è¯•å¹¶è¡Œè¯·æ±‚
curl -X POST http://localhost:10601/v1/chat/completions \
  -H "Authorization: Bearer cat-xxx" \
  -d '{"model": "gemini-2.5-flash", "messages": [{"role": "user", "content": "test"}]}'
```

---

## ğŸ”„ å›æ»šæ–¹æ¡ˆ

### å¦‚æœå‡ºç°é—®é¢˜

```bash
# 1. å…³é—­å¹¶è¡Œæ¨¡å¼
# ç¼–è¾‘ .env
ENABLE_PARALLEL_ENDPOINTS=false

# 2. é‡å¯æœåŠ¡
docker-compose restart backend

# 3. æˆ–å›æ»šä»£ç 
git checkout HEAD~1
docker-compose up -d --build
```

---

## ğŸ“ æ€»ç»“

### æ¨èæ–¹æ¡ˆ: **æ–¹æ¡ˆ Aï¼ˆç«é€Ÿæ¨¡å¼ï¼‰**

**ç†ç”±**:
1. âœ… å“åº”é€Ÿåº¦æœ€å¿«ï¼ˆæå‡ 60%+ï¼‰
2. âœ… å¯ç”¨æ€§æœ€é«˜ï¼ˆ99.9%ï¼‰
3. âœ… è‡ªåŠ¨å®¹é”™ï¼Œæ— éœ€äººå·¥å¹²é¢„
4. âœ… å®ç°ç›¸å¯¹ç®€å•

**æ³¨æ„äº‹é¡¹**:
1. âš ï¸ ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
2. âš ï¸ é…ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
3. âš ï¸ ä¸ gcli2api å›¢é˜Ÿåè°ƒï¼Œç¡®ä¿èƒ½æ‰¿å—å‹åŠ›

### å®æ–½ä¼˜å…ˆçº§

1. **Phase 1**: åˆ é™¤ `ag-` å‰ç¼€ï¼Œç»Ÿä¸€æ¨¡å‹åˆ—è¡¨
2. **Phase 2**: å®ç°å¹¶è¡Œç«é€Ÿé€»è¾‘
3. **Phase 3**: æ·»åŠ ç›‘æ§å’Œæ—¥å¿—
4. **Phase 4**: æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜

---

**æ–¹æ¡ˆç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2026-01-09
**ä½œè€…**: Claude Sonnet 4.5
